{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Designer (Data Prep)\n",
    "\n",
    "In this exercise we will be building a pipeline in Azure Machine Learning using the [Visual Designer](https://docs.microsoft.com/azure/machine-learning/concept-designer). Traditionally the Visual Designer is used for training and deploying models. Here we will build a data prep pipeline that get a dataset ready for downstream model scoring. Below you can see a final picture of the data prep pipeline that will be built as part of this exercise.\n",
    "\n",
    "The pipeline will join two datasets together that consists of the diabetes dataset. We will perform binning on the Age column. After joining the datasets together, we will use the [SQL Transformation](https://docs.microsoft.com/azure/machine-learning/component-reference/apply-sql-transformation) component to demonstrate the flexibility of the Visual Designer by creating an aggregate dataset. The resulting datasets will be landed in the /1-bronze folder of the data lake. Later we will build in a scoring pipeline that will use the result dataset.\n",
    "\n",
    "![Final data prep pipeline in Visual Designer](./img/vddataprepfinal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Stage data\n",
    "\n",
    "Let's first upload our source files to the /0-raw layer of the data lake. We will use this as the source for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Supply userid value for naming artifacts.\n",
    "userid = ''\n",
    "\n",
    "tabular_dataset_name = 'diabetes-data-bronze-' + userid\n",
    "\n",
    "print(\n",
    "tabular_dataset_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "# Set datastore name where raw diabetes data is stored.\n",
    "datastore_name = ''\n",
    "\n",
    "datastore = Datastore.get(ws, datastore_name)\n",
    "print(\"Found Datastore with name: %s\" % datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "# Upload local csv files to ADLS using AML Datastore.\n",
    "ds = Dataset.File.upload_directory(src_dir='../data/stage',\n",
    "           target=DataPath(datastore,  '0-raw/diabetes/' + userid + '/stage/'),\n",
    "           show_progress=True)\n",
    "\n",
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create target datasets\n",
    "Register datasets to use as targets for writing data from pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_ds = Dataset.Tabular.from_delimited_files(path=(datastore,'1-bronze/diabetes/' + userid + '/diabetes.csv'),validate=False,infer_column_types=False)\n",
    "diabetes_ds.register(ws,name=tabular_dataset_name,create_new_version=True)\n",
    "\n",
    "diabetes_ds = Dataset.Tabular.from_delimited_files(path=(datastore,'1-bronze/diabetes/' + userid + '/diabetes_sql_example.csv'),validate=False,infer_column_types=False)\n",
    "diabetes_ds.register(ws,name=tabular_dataset_name + '_sql_example',create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create new pipeline\n",
    "\n",
    "In the Azure ML studio, navigate to <b>Designer</b> and press the <b>+</b> button under <b>New pipeline</b>\n",
    "\n",
    "![Screenshot of AML Studio highlighting the steps described to create a new pipeline](./img/vdnewpipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In <b>Settings</b> change the compute type to <b>Compute cluster</b> and select the appropriate compute cluster.\n",
    "1. Name the pipeline in the <b>Draft name</b> field using the convention \"pipeline-data-prep-diabetes-<userid>-prod\"\n",
    "\n",
    "![Settings pane with compute settings and draft name fields highlighted](./img/vdsettingpipelinename.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Input and Output</b> from the components menu.\n",
    "2. Drag <b>Import Data</b> onto the canvas.\n",
    "3. Change the <b>Data source</b> to <b>URL via HTTP</b>\n",
    "4. Enter the storage url to the <b>patient-age.csv</b> file in the <b>/0-raw</b> folder of the data lake.\n",
    "5. Validate by pressing <b>Preview schema</b>\n",
    "\n",
    "![Import data component for patient-age.csv](./img/vdimportpatientage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Input and Output</b> from the components menu.\n",
    "2. Drag <b>Import Data</b> onto the canvas.\n",
    "3. Change the <b>Data source</b> to <b>URL via HTTP</b>\n",
    "4. Enter the storage url to the <b>patient-levels.csv</b> file in the <b>/0-raw</b> folder of the data lake.\n",
    "5. Validate by pressing <b>Preview schema</b>\n",
    "\n",
    "![Import data component for patient-levels.csv](./img/vdimportpatientlevels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Transformation</b> from the components menu.\n",
    "2. Drag <b>Group Data into Bins</b> onto the canvas.\n",
    "3. Connect <b>Import Data</b> for patient-age.csv.\n",
    "4. Change <b>Binning mode</b> to <b>Custom Edges</b>.\n",
    "5. Paste the following value in the <b>Comma-separated list of bin edges</b> field. \"1,11,21,31,41,51,61,71,81,91\"\n",
    "6. Select the <b>Age</b> column for <b>Columns to bin</b>.\n",
    "\n",
    "![Group Data into Bins component settings](./img/vdbindata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Transformation</b> from the components menu.\n",
    "2. Drag <b>Join Data</b> onto the canvas.\n",
    "3. Connect <b>Group Data into Bins</b> component using the <b>Quantized dataset: DataFrameDirectory</b> output to <b>Join Data</b> component <b>Left dataset: DataFrameDirectory</b> input.\n",
    "4. Connect <b>Import Data</b> for patient-levels.csv to <b>Join Data</b> component <b>Right dataset: DataFrameDirectory</b> input.\n",
    "5. Set the right and left join key columns to <b>Id</b>\n",
    "6. Leave defaults as shown in screenshot.\n",
    "\n",
    "![Join data component settings](./img/vdjoindata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Transformation</b> from the components menu.\n",
    "2. Drag <b>Select Columns in Dataset</b> onto the canvas.\n",
    "3. Connect <b>Join Data</b> to <b>Select Columns in Dataset</b>.\n",
    "4. Add the following columns to <b>Select columns</b>. \"Id,PatientID,Age,Age_quantized,Pregnancies,PlasmaGlucose,DiastolicBloodPressure,TricepsThickness,SerumInsulin,BMI,DiabetesPedigree\"\n",
    "\n",
    "![Select Columns in Dataset settings](./img/vdselectcolumns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Input and Output</b> from the components menu.\n",
    "2. Drag <b>Export Data</b> onto the canvas.\n",
    "3. Connect <b>Select Columns in Dataset</b> to <b>Export Data</b>.\n",
    "4. Choose <b>Azure Data Lake Storage Gen2</b> from the <b>Datastore type</b> dropdown.\n",
    "5. Select the workshop datastore from the <b>Datastore</b> dropdown.\n",
    "6. Enter the path to the <b>/1-bronze</b> diabetes folder with filename <b>diabetes.csv</b>\n",
    "7. Choose <b>csv</b> for the <b>File format</b>.\n",
    "\n",
    "![Export Data component settings for diabetes.csv](./img/vdexportdiabetesdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Transformation</b> from the components menu.\n",
    "2. Drag <b>Apply SQL Transformation</b> onto the canvas.\n",
    "3. Connect <b>Join Data</b> to the <b>Apply SQL Transformation</b> input <b>t1: DataFrameDirectory</b>.\n",
    "4. Enter the following SQL statement in the <b>SQL query script</b> field.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "PatientID\n",
    ",MAX(BMI) \n",
    "FROM t1\n",
    "GROUP BY PatientID\n",
    "```\n",
    "![Apply SQL Transformation component settings](./img/vdsqltransformation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open <b>Data Input and Output</b> from the components menu.\n",
    "2. Drag <b>Export Data</b> onto the canvas.\n",
    "3. Connect <b>Apply SQL Transformation</b> to <b>Export Data</b>.\n",
    "4. Choose <b>Azure Data Lake Storage Gen2</b> from the <b>Datastore type</b> dropdown.\n",
    "5. Select the workshop datastore from the <b>Datastore</b> dropdown.\n",
    "6. Enter the path to the <b>/1-bronze</b> diabetes folder with filename <b>diabetes_sql_example.csv</b>\n",
    "7. Choose <b>csv</b> for the <b>File format</b>.\n",
    "\n",
    "![Export Data component settings for diabetes.csv](./img/vdexportdiabetessqldata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Submit and Publish pipeline\n",
    "First submit the pipeline and ensure it runs as expected. Second publish the pipeline endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Press <b>Submit</b>\n",
    "2. Choose <b>Create New</b> for Experiment.\n",
    "3. Name the new experiment using this convention. \"pipeline-data-prep-diabetes-<'userid'>-prod\"\n",
    "4. Press the <b>Submit</b> button.\n",
    "5. Monitor the run for completion.\n",
    "\n",
    "![Set up pipeline run settings](./img/vdsubmitpipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Verify <b>diabetes.csv</b> and <b>diabetes_sql_example.csv</b> are created after pipeline run in <b>/1-bronze</b> folder.\n",
    "\n",
    "![screenshot of Storage Explorer showing output files from pipeline run](./img/vdoutputfiles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Verify registered datasets recognize the new files\n",
    "\n",
    "![dataset overview in AML studio with Files in dataset highlighted showing 1](./img/vddatasetoutput.png)\n",
    "\n",
    "![dataset overview in AML studio explore showing data sample](./img/vdexploredatasetoutput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open the pipeline and press the <b>Publish</b> button.\n",
    "2. Choose <b>Create new</b> and name the pipeline endpoint the same as the pipeline draft.\n",
    "3. Press the <b>Publish</b> button.\n",
    "\n",
    "![The Set up published pipeline menu in the AML Studio Visual Designer](./img/vdpublishpipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "\n",
    "This data prep pipeline will be orchestrated using Azure Data Factory with scoring and training pipelines that are published in Module 4. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4c6afd7f173e0e931b0843fe6a0c2993fb0269572e807aff8dc4b63e60c94bb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
